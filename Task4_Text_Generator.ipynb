{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6w6osSPTwqF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Initializing Model... (This may take a minute)\")\n",
        "\n",
        "# Check for GPU availability for faster processing\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "# Load pre-trained model and tokenizer (GPT2 Medium for better quality)\n",
        "try:\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
        "    model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")\n",
        "    model.to(device)\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "\n",
        "\n",
        "print(\"\\nModel Loaded! Ready to generate text.\\n\")"
      ],
      "metadata": {
        "id": "AMpBSejqUXGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Instructions: Enter a topic or start of a sentence.\")\n",
        "print(\"Type 'exit' to quit.\\n\")\n",
        "\n",
        "# Interactive Loop\n",
        "while True:\n",
        "  prompt = input(\">> Enter Topic/Prompt: \")\n",
        "\n",
        "  if prompt.lower() in ['exit', 'quit']:\n",
        "      print(\"Exiting...\")\n",
        "      break\n",
        "\n",
        "  if not prompt.strip():\n",
        "      continue\n",
        "\n",
        "  print(\"\\n...Generating Paragraph...\\n\")\n",
        "\n",
        "  # Encode input\n",
        "  input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
        "  attention_mask = torch.ones(input_ids.shape, device=device)\n",
        "\n",
        "  # Generate Text\n",
        "  with torch.no_grad():\n",
        "      output = model.generate(\n",
        "          input_ids,\n",
        "          attention_mask=attention_mask,\n",
        "          max_length=200,           # Maximum length of the paragraph\n",
        "          temperature=0.8,          # Creativity level (0.7-0.9 is best)\n",
        "          top_k=50,                 # Top-K sampling\n",
        "          top_p=0.95,               # Nucleus sampling\n",
        "          do_sample=True,           # specific setting for creative text\n",
        "          repetition_penalty=1.2,   # Prevents repeating phrases\n",
        "          no_repeat_ngram_size=2,   # Prevents repeating 2-word combinations\n",
        "          pad_token_id=tokenizer.eos_token_id\n",
        "      )\n",
        "\n",
        "  # Decode output\n",
        "  generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "  print(\"-\" * 50)\n",
        "  print(generated_text)\n",
        "  print(\"-\" * 50 + \"\\n\")"
      ],
      "metadata": {
        "id": "KbPJbSdJVUev"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}